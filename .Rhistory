npv <- if (tn + fn > 0) tn / (tn + fn) else 0  # Negative Predictive Value
group_metrics <- rbind(group_metrics, data.frame(
model = i,
group = g,
accuracy = accuracy,
tpr = tpr,
tnr = tnr,
fpr = fpr,
fnr = fnr,
ppv = ppv,
npv = npv,
tp = tp,
tn = tn,
fp = fp,
fn = fn
))
}
individual_metrics_by_group[[i]] <- group_metrics
all_pred_classes[[i]] <- pred_cls
}
# Combine individual model results
individual_metrics_df <- do.call(rbind, individual_metrics_by_group)
# Mean metrics across models for each group
mean_metrics <- aggregate(
cbind(accuracy, tpr, tnr, fpr, fnr, ppv, npv) ~ group,
data = individual_metrics_df,
FUN = mean
)
# Ensemble predictions (majority voting)
pred_mat <- do.call(cbind, all_pred_classes)
ensemble_cls <- ifelse(rowMeans(pred_mat) > threshold, 2, 1)
# Ensemble metrics by group
unique_groups <- unique(groups)
ensemble_metrics <- data.frame()
for (g in unique_groups) {
mask <- groups == g
y_g <- y_true[mask]
pred_g <- ensemble_cls[mask]
# Confusion matrix
tp <- sum(y_g == positive_class & pred_g == positive_class)
tn <- sum(y_g != positive_class & pred_g != positive_class)
fp <- sum(y_g != positive_class & pred_g == positive_class)
fn <- sum(y_g == positive_class & pred_g != positive_class)
# Rates
tpr <- if (tp + fn > 0) tp / (tp + fn) else 0
tnr <- if (tn + fp > 0) tn / (tn + fp) else 0
fpr <- if (fp + tn > 0) fp / (fp + tn) else 0
fnr <- if (fn + tp > 0) fn / (fn + tp) else 0
accuracy <- (tp + tn) / (tp + tn + fp + fn)
ppv <- if (tp + fp > 0) tp / (tp + fp) else 0
npv <- if (tn + fn > 0) tn / (tn + fn) else 0
ensemble_metrics <- rbind(ensemble_metrics, data.frame(
group = g,
accuracy = accuracy,
tpr = tpr,
tnr = tnr,
fpr = fpr,
fnr = fnr,
ppv = ppv,
npv = npv,
tp = tp,
tn = tn,
fp = fp,
fn = fn
))
}
# Calculate fairness measures (following paper's Section 5.3)
if (is.null(levels_order)) {
uniq_groups <- as.character(sort(unique(groups)))
if (length(uniq_groups) != 2) {
stop("Need exactly two groups for fairness measures")
}
levels_order <- uniq_groups
}
# Create lookup for ensemble metrics
ens_lookup <- setNames(
split(ensemble_metrics, ensemble_metrics$group),
ensemble_metrics$group
)
if (!all(levels_order %in% names(ens_lookup))) {
stop("levels_order values not found in group data")
}
minority_metrics <- ens_lookup[[levels_order[1]]]
majority_metrics <- ens_lookup[[levels_order[2]]]
# Fairness measures (Equations 23-26 from paper)
fairness_measures <- data.frame(
minority = levels_order[1],
majority = levels_order[2],
# Statistical Parity: P(킷=1|G=0) / P(킷=1|G=1)
F_SP = (minority_metrics$tp + minority_metrics$fp) / sum(minority_metrics[c("tp", "tn", "fp", "fn")]) /
((majority_metrics$tp + majority_metrics$fp) / sum(majority_metrics[c("tp", "tn", "fp", "fn")])),
# Equal Opportunity: FNR(G=0) / FNR(G=1)
F_EOpp = minority_metrics$fnr / majority_metrics$fnr,
# Equal Accuracy: Acc(G=0) / Acc(G=1)
F_EAcc = minority_metrics$accuracy / majority_metrics$accuracy,
# Additional rate ratios for analysis
TPR_ratio = minority_metrics$tpr / majority_metrics$tpr,
TNR_ratio = minority_metrics$tnr / majority_metrics$tnr,
FPR_ratio = minority_metrics$fpr / majority_metrics$fpr,
FNR_ratio = minority_metrics$fnr / majority_metrics$fnr
)
list(
individual_metrics_by_group = individual_metrics_df,
mean_individual_metrics_by_group = mean_metrics,
ensemble_metrics_by_group = ensemble_metrics,
fairness_measures = fairness_measures,
confusion_matrices = list(
minority = minority_metrics[c("tp", "tn", "fp", "fn")],
majority = majority_metrics[c("tp", "tn", "fp", "fn")]
)
)
}
results <- calculate_ensemble_metrics_by_group(
models = your_bnn_models,
test_data = your_test_data,
target_col = "y",
group_col = "G",
levels_order = c("minority", "majority"),
positive_class = 2
)
calculate_ensemble_metrics_by_group <- function(models,
test_data,
target_col,
group_col = "G",
threshold = 1.5,
levels_order = NULL,
positive_class = 2) {
stopifnot(length(models) > 0)
td <- test_data
# Extract y and G
y_true <- td[[target_col]]
groups <- td[[group_col]]
if (is.factor(groups)) groups <- droplevels(groups)
# Individual model predictions
all_pred_classes <- vector("list", length(models))
individual_metrics_by_group <- list()
for (i in seq_along(models)) {
raw_pred <- predict(models[[i]], newdata = td)
pred_cls <- ifelse(raw_pred > threshold, 2, 1)
# Calculate metrics for each group
unique_groups <- unique(groups)
group_metrics <- data.frame()
for (g in unique_groups) {
mask <- groups == g
y_g <- y_true[mask]
pred_g <- pred_cls[mask]
# Confusion matrix components
tp <- sum(y_g == positive_class & pred_g == positive_class)
tn <- sum(y_g != positive_class & pred_g != positive_class)
fp <- sum(y_g != positive_class & pred_g == positive_class)
fn <- sum(y_g == positive_class & pred_g != positive_class)
# Calculate rates
tpr <- if (tp + fn > 0) tp / (tp + fn) else 0  # True Positive Rate
tnr <- if (tn + fp > 0) tn / (tn + fp) else 0  # True Negative Rate
fpr <- if (fp + tn > 0) fp / (fp + tn) else 0  # False Positive Rate
fnr <- if (fn + tp > 0) fn / (fn + tp) else 0  # False Negative Rate
# Additional metrics from paper
accuracy <- (tp + tn) / (tp + tn + fp + fn)
ppv <- if (tp + fp > 0) tp / (tp + fp) else 0  # Positive Predictive Value
npv <- if (tn + fn > 0) tn / (tn + fn) else 0  # Negative Predictive Value
group_metrics <- rbind(group_metrics, data.frame(
model = i,
group = g,
accuracy = accuracy,
tpr = tpr,
tnr = tnr,
fpr = fpr,
fnr = fnr,
ppv = ppv,
npv = npv,
tp = tp,
tn = tn,
fp = fp,
fn = fn
))
}
individual_metrics_by_group[[i]] <- group_metrics
all_pred_classes[[i]] <- pred_cls
}
# Combine individual model results
individual_metrics_df <- do.call(rbind, individual_metrics_by_group)
# Mean metrics across models for each group
mean_metrics <- aggregate(
cbind(accuracy, tpr, tnr, fpr, fnr, ppv, npv) ~ group,
data = individual_metrics_df,
FUN = mean
)
# Ensemble predictions (majority voting)
pred_mat <- do.call(cbind, all_pred_classes)
ensemble_cls <- ifelse(rowMeans(pred_mat) > threshold, 2, 1)
# Ensemble metrics by group
unique_groups <- unique(groups)
ensemble_metrics <- data.frame()
for (g in unique_groups) {
mask <- groups == g
y_g <- y_true[mask]
pred_g <- ensemble_cls[mask]
# Confusion matrix
tp <- sum(y_g == positive_class & pred_g == positive_class)
tn <- sum(y_g != positive_class & pred_g != positive_class)
fp <- sum(y_g != positive_class & pred_g == positive_class)
fn <- sum(y_g == positive_class & pred_g != positive_class)
# Rates
tpr <- if (tp + fn > 0) tp / (tp + fn) else 0
tnr <- if (tn + fp > 0) tn / (tn + fp) else 0
fpr <- if (fp + tn > 0) fp / (fp + tn) else 0
fnr <- if (fn + tp > 0) fn / (fn + tp) else 0
accuracy <- (tp + tn) / (tp + tn + fp + fn)
ppv <- if (tp + fp > 0) tp / (tp + fp) else 0
npv <- if (tn + fn > 0) tn / (tn + fn) else 0
ensemble_metrics <- rbind(ensemble_metrics, data.frame(
group = g,
accuracy = accuracy,
tpr = tpr,
tnr = tnr,
fpr = fpr,
fnr = fnr,
ppv = ppv,
npv = npv,
tp = tp,
tn = tn,
fp = fp,
fn = fn
))
}
# Calculate fairness measures (following paper's Section 5.3)
if (is.null(levels_order)) {
uniq_groups <- as.character(sort(unique(groups)))
if (length(uniq_groups) != 2) {
stop("Need exactly two groups for fairness measures")
}
levels_order <- uniq_groups
}
# Create lookup for ensemble metrics
ens_lookup <- setNames(
split(ensemble_metrics, ensemble_metrics$group),
ensemble_metrics$group
)
if (!all(levels_order %in% names(ens_lookup))) {
stop("levels_order values not found in group data")
}
minority_metrics <- ens_lookup[[levels_order[1]]]
majority_metrics <- ens_lookup[[levels_order[2]]]
# Fairness measures (Equations 23-26 from paper)
fairness_measures <- data.frame(
minority = levels_order[1],
majority = levels_order[2],
# Statistical Parity: P(킷=1|G=0) / P(킷=1|G=1)
F_SP = (minority_metrics$tp + minority_metrics$fp) / sum(minority_metrics[c("tp", "tn", "fp", "fn")]) /
((majority_metrics$tp + majority_metrics$fp) / sum(majority_metrics[c("tp", "tn", "fp", "fn")])),
# Equal Opportunity: FNR(G=0) / FNR(G=1)
F_EOpp = minority_metrics$fnr / majority_metrics$fnr,
# Equal Accuracy: Acc(G=0) / Acc(G=1)
F_EAcc = minority_metrics$accuracy / majority_metrics$accuracy,
# Additional rate ratios
TPR_ratio = minority_metrics$tpr / majority_metrics$tpr,
TNR_ratio = minority_metrics$tnr / majority_metrics$tnr,
FPR_ratio = minority_metrics$fpr / majority_metrics$fpr,
FNR_ratio = minority_metrics$fnr / majority_metrics$fnr
)
list(
individual_metrics_by_group = individual_metrics_df,
mean_individual_metrics_by_group = mean_metrics,
ensemble_metrics_by_group = ensemble_metrics,
fairness_measures = fairness_measures,
confusion_matrices = list(
minority = minority_metrics[c("tp", "tn", "fp", "fn")],
majority = majority_metrics[c("tp", "tn", "fp", "fn")]
)
)
}
results <- calculate_ensemble_metrics_by_group(
models = your_bnn_models,
test_data = your_test_data,
target_col = "y",
group_col = "G",
levels_order = c("minority", "majority"),
positive_class = 2
)
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("minority", "majority"),
positive_class = 2
)
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
results$fairness_measures          # All fairness ratios
results$confusion_matrices         # TP, TN, FP, FN counts
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
results$fairness_measures          # All fairness ratios
results$confusion_matrices         # TP, TN, FP, FN counts
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
uncertainties <- calculate_uncertainties(uncertainty_bnn, test_data)
fairness_measures <- calculate_uncertainty_fairness(uncertainties, test_data$G)
print(fairness_measures)
sensitive_attr <- test_data$G
# fairness <- calculate_fairness_measures(uncertainties, sensitive_attr)
fairness_epistemic <- calculate_fairness_measures(uncertainties$epistemic, sensitive_attr)
fairness_aleatoric <- calculate_fairness_measures(uncertainties$aleatoric, sensitive_attr)
fairness_predictive <- calculate_fairness_measures(uncertainties$predictive, sensitive_attr)
# Funzione per creare ensemble che simula Monte Carlo sampling
create_uncertainty_ensemble <- function(data, target_col, n_models = 10) {
models <- list()
# Crea formula dinamicamente
formula_str <- paste(target_col, "~ .")
for(i in 1:n_models) {
cat("Training model", i, "of", n_models, "\n")
# Bootstrap sampling (per aleatoric)
boot_indices <- sample(nrow(data), nrow(data), replace = TRUE)
boot_data <- data[boot_indices, ]
# Aggiungi noise ai parametri (per epistemic)
set.seed(i)  # diversi seed per diversi modelli
# SOSTITUITO: Allena BNN con bnns
model <- bnns(
formula = as.formula(formula_str),
data = boot_data,
hidden_layers = 0,  # varia architettura
epochs = 5,
seed = i
)
models[[i]] <- model
}
return(models)
}
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
test_data <- test_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan
test_data$y <- as.numeric(test_data$y)
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
test_data <- test_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan
test_data$y <- as.numeric(test_data$y)
uncertainty_bnn <- create_uncertainty_ensemble(train_data, 'y')
# Opzione 2: Calcolo completo con ensemble
accuracy_results <- calculate_ensemble_accuracy(uncertainty_bnn, test_data, 'y')
# Opzione 2: Calcolo completo con ensemble
accuracy_results <- calculate_ensemble_accuracy(uncertainty_bnn, test_data, 'y')
```{r}
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
results$fairness_measures          # All fairness ratios
results$confusion_matrices         # TP, TN, FP, FN counts
uncertainties <- calculate_uncertainties(uncertainty_bnn, test_data)
uncertainties <- calculate_uncertainties(uncertainty_bnn, test_data)
fairness_measures <- calculate_uncertainty_fairness(uncertainties, test_data$G)
fairness_measures <- calculate_uncertainty_fairness(uncertainties, test_data$G)
print(fairness_measures)
source("bnn.R")
source("gigi_puzza.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/02_synthetic_data.R")
# Prepara train/test split
set.seed(42)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
test_data <- test_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan
test_data$y <- as.numeric(test_data$y)
uncertainty_bnn <- create_uncertainty_ensemble(train_data, 'y')
# Opzione 2: Calcolo completo con ensemble
accuracy_results <- calculate_ensemble_accuracy(uncertainty_bnn, test_data, 'y')
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
results$fairness_measures          # All fairness ratios
results$confusion_matrices         # TP, TN, FP, FN counts
uncertainties <- calculate_uncertainties(uncertainty_bnn, test_data)
fairness_measures <- calculate_uncertainty_fairness(uncertainties, test_data$G)
print(fairness_measures)
source("bnn.R")
source("gigi_puzza.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/02_synthetic_data.R")
# Prepara train/test split
set.seed(42)
train_indices <- sample(nrow(sd2_data), 0.8 * nrow(sd2_data))
train_data <- sd2_data[train_indices, ]
test_data <- sd2_data[-train_indices, ]
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
test_data <- test_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan
test_data$y <- as.numeric(test_data$y)
uncertainty_bnn <- create_uncertainty_ensemble(train_data, 'y')
# Opzione 2: Calcolo completo con ensemble
accuracy_results <- calculate_ensemble_accuracy(uncertainty_bnn, test_data, 'y')
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
results$fairness_measures          # All fairness ratios
results$confusion_matrices         # TP, TN, FP, FN counts
uncertainties <- calculate_uncertainties(uncertainty_bnn, test_data)
fairness_measures <- calculate_uncertainty_fairness(uncertainties, test_data$G)
print(fairness_measures)
source("bnn.R")
source("gigi_puzza.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/03_synthetic_data.R")
# Prepara train/test split
set.seed(42)
train_indices <- sample(nrow(sd3_data), 0.8 * nrow(sd2_data))
train_data <- sd3_data[train_indices, ]
test_data <- sd3_data[-train_indices, ]
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
test_data <- test_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan
test_data$y <- as.numeric(test_data$y)
uncertainty_bnn <- create_uncertainty_ensemble(train_data, 'y')
# Carica il dataset
adult_data <- read.csv("adult.data", header = FALSE, sep = ",", strip.white = TRUE, na.strings = "?")
# Opzione 2: Calcolo completo con ensemble
accuracy_results <- calculate_ensemble_accuracy(uncertainty_bnn, test_data, 'y')
results <- calculate_ensemble_metrics_by_group(
models = uncertainty_bnn,
test_data = test_data,
target_col = "y",
group_col = "G",
levels_order = c("0", "1"),
positive_class = 2
)
# Access results
results$ensemble_metrics_by_group  # TPR, FPR, etc. by group
results$fairness_measures          # All fairness ratios
results$confusion_matrices         # TP, TN, FP, FN counts
uncertainties <- calculate_uncertainties(uncertainty_bnn, test_data)
fairness_measures <- calculate_uncertainty_fairness(uncertainties, test_data$G)
print(fairness_measures)
# Load necessary libraries
library(readr)
# Read the data
adult_data <- read_csv('adult.data', col_names = FALSE)
# Read the data
adult_data <- read_csv('data/real_datasets/adult/adult.data', col_names = FALSE)
# Load necessary libraries
library(readr)
# Read the data
adult_data <- read_csv('data/real_datasets/adult/adult.data', col_names = FALSE)
adult_test <- read_csv('adult.test', skip = 1, col_names = FALSE)
# Read the data
adult_data <- read_csv('data/real_datasets/adult/adult.data', col_names = FALSE)
colnames(adult_test) <- c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income")
# View summary
summary(adult_data)
summary(adult_test)
colnames(adult_test) <- c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income")
adult_test <- read_csv('data/real_datasets/adult/adult.test', skip = 1, col_names = FALSE)
# Assign column names
colnames(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income")
colnames(adult_test) <- c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income")
# View summary
summary(adult_data)
summary(adult_test)
adult_data
train_data <- read_csv('data/real_datasets/adult/adult.data', col_names = FALSE)
test_data <- read_csv('data/real_datasets/adult/adult.test', skip = 1, col_names = FALSE)
