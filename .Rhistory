# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
ensemble_models <- create_uncertainty_ensemble(train_data, 'Y')
View(ensemble_models)
View(ensemble_models)
View(ensemble_models)
ensemble_models[]
àù++
View(ensemble_models)
ensemble_models[[1]]
uncertainties <- calculate_uncertainties(ensemble_models, test_data)
uncertainties[[1]]
uncertainties[[2]]
View(uncertainties)
uncertainties[[3]]
uncertainties[["epistemic"]]
fairness <- calculate_fairness_measures(uncertainties, 'Y')
View(fairness)
fairness <- calculate_fairness_measures(uncertainties, sensitive_attr)
---
title: "Main analysis"
```{r setup}
source("bnn.R")
source("bnn.R")
source("esemble.R")
source("fairness.R")
source("uncertainties.R")
source("bnn.R")
source("ensemble.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/01_synthetic_data.R")
```{r}
# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
```{r}
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
create_uncertainty_ensemble <- create_uncertainty_ensemble(train_data, 'y')
# Funzione per creare ensemble che simula Monte Carlo sampling
create_uncertainty_bnn <- function(data, target_col, n_models = 3) {
models <- list()
# Crea formula dinamicamente
formula_str <- paste(target_col, "~ .")
for(i in 1:n_models) {
cat("Training model", i, "of", n_models, "\n")
# Bootstrap sampling (per aleatoric)
boot_indices <- sample(nrow(data), nrow(data), replace = TRUE)
boot_data <- data[boot_indices, ]
# Aggiungi noise ai parametri (per epistemic)
set.seed(i)  # diversi seed per diversi modelli
# SOSTITUITO: Allena BNN con bnns
model <- bnns(
formula = as.formula(formula_str),
data = boot_data,
hidden_layers = 10,  # varia architettura
epochs = 50,               # varia training
seed = i
)
models[[i]] <- model
}
return(models)
}
source("bnn.R")
source("ensemble.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/01_synthetic_data.R")
source("bnn.R")
source("ensemble.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/01_synthetic_data.R")
```{r}
# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
```{r}
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
create_uncertainty_bnn <- create_uncertainty_bnn(train_data, 'y')
# metriche
group_metrics <- calculate_group_metrics(create_uncertainty_bnn, test_data, sensitive_attr = "G")
View(create_uncertainty_bnn)
create_uncertainty_bnn <- create_uncertainty_bnn(train_data, 'y')
source("fairness.R")
create_uncertainty_bnn
# Funzione per calcolare le metriche
compute_metrics <- function(y_true, y_pred) {
cm <- table(factor(y_true, levels = c(0,1)), factor(y_pred, levels = c(0,1)))
tn <- cm[1,1]
fp <- cm[1,2]
fn <- cm[2,1]
tp <- cm[2,2]
accuracy <- (tp + tn) / sum(cm)
tpr <- tp / (tp + fn)
tnr <- tn / (tn + fp)
fpr <- fp / (fp + tn)
fnr <- fn / (fn + tp)
return(c(accuracy = accuracy, TPR = tpr, TNR = tnr, FPR = fpr, FNR = fnr))
}
library(dplyr)
library(caret)  # per confusionMatrix
group_metrics <- test_data %>%
group_by(G) %>%
summarise(metrics = list(compute_metrics(y_true, y_pred))) %>%
unnest_wider(metrics)
print(group_metrics)
group_metrics <- test_data %>%
group_by(G) %>%
summarise(metrics = list(compute_metrics(y_true, y_pred))) %>%
unnest_wider(metrics)
# Calcolo delle metriche per gruppo
group_metrics <- calculate_group_metrics(test_data, true_col = "y", pred_col = "y_pred", group_col = "G")
# Calcola metriche di accuratezza per ciascun gruppo
calculate_group_metrics <- function(data, true_col = "y", pred_col = "y_pred", group_col = "G") {
library(dplyr)
data %>%
group_by(!!sym(group_col)) %>%
summarise(
TP = sum(!!sym(true_col) == 1 & !!sym(pred_col) == 1),
TN = sum(!!sym(true_col) == 0 & !!sym(pred_col) == 0),
FP = sum(!!sym(true_col) == 0 & !!sym(pred_col) == 1),
FN = sum(!!sym(true_col) == 1 & !!sym(pred_col) == 0),
TPR = TP / (TP + FN),
TNR = TN / (TN + FP),
FPR = FP / (FP + TN),
FNR = FN / (FN + TP)
) %>%
ungroup()
}
# Calcolo delle metriche per gruppo
group_metrics <- calculate_group_metrics(test_data, true_col = "y", pred_col = "y_pred", group_col = "G")
# Calcola metriche di accuratezza per ciascun gruppo
calculate_group_metrics <- function(data, true_col = "y", pred_col = "y_pred", group_col = "G") {
library(dplyr)
data %>%
group_by(across(all_of(group_col))) %>%
summarise(
TP = sum(data[[true_col]] == 1 & data[[pred_col]] == 1),
TN = sum(data[[true_col]] == 0 & data[[pred_col]] == 0),
FP = sum(data[[true_col]] == 0 & data[[pred_col]] == 1),
FN = sum(data[[true_col]] == 1 & data[[pred_col]] == 0),
TPR = ifelse((TP + FN) > 0, TP / (TP + FN), NA),
TNR = ifelse((TN + FP) > 0, TN / (TN + FP), NA),
FPR = ifelse((FP + TN) > 0, FP / (FP + TN), NA),
FNR = ifelse((FN + TP) > 0, FN / (FN + TP), NA)
) %>%
ungroup()
}
# Calcolo delle metriche per gruppo
group_metrics <- calculate_group_metrics(test_data, true_col = "y", pred_col = "y_pred", group_col = "G")
print(group_metrics)
# Calcola metriche di accuratezza per ciascun gruppo
calculate_group_metrics <- function(data, true_col = "y", pred_col = "y_pred", group_col = "G") {
library(dplyr)
data %>%
group_by(.data[[group_col]]) %>%
summarise(
TP = sum(.data[[true_col]] == 1 & .data[[pred_col]] == 1),
TN = sum(.data[[true_col]] == 0 & .data[[pred_col]] == 0),
FP = sum(.data[[true_col]] == 0 & .data[[pred_col]] == 1),
FN = sum(.data[[true_col]] == 1 & .data[[pred_col]] == 0),
TPR = ifelse((TP + FN) > 0, TP / (TP + FN), NA_real_),
TNR = ifelse((TN + FP) > 0, TN / (TN + FP), NA_real_),
FPR = ifelse((FP + TN) > 0, FP / (FP + TN), NA_real_),
FNR = ifelse((FN + TP) > 0, FN / (FN + TP), NA_real_)
) %>%
ungroup()
}
test_data$y <- as.numeric(test_data$Y)
test_data$y_pred <- as.numeric(round(predict(models[[1]], newdata = test_data)))  # esempio con primo modello
# Calcolo delle metriche per gruppo
group_metrics <- calculate_group_metrics(test_data, true_col = "y", pred_col = "y_pred", group_col = "G")
test_data$y <- as.numeric(test_data$Y)
View(create_uncertainty_bnn)
```
```
# Calcolo delle metriche per gruppo
group_accuracy <- calculate_group_accuracy(create_uncertainty_bnn, test_data, "G")
# Calcolo delle metriche per gruppo
group_accuracy <- calculate_group_accuracy(create_uncertainty_bnn, test_data, sensitive_attr = "G")
# Calcolo delle metriche per gruppo
group_accuracy <- calculate_group_accuracy(create_uncertainty_bnn, test_data, sensitive_attr = "Y")
print(group_metrics)
# Calcolo delle metriche per gruppo
group_accuracy <- calculate_group_accuracy(create_uncertainty_bnn, test_data, sensitive_attr = "y")
View(sd1_data)
source("bnn.R")
source("ensemble.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/01_synthetic_data.R")
source("bnn.R")
source("ensemble.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/01_synthetic_data.R")
source("bnn.R")
source("ensemble.R")
source("fairness.R")
source("uncertainties.R")
source("data/syntethic_datasets/01_synthetic_data.R")
```{r}
# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
# Prepara train/test split
set.seed(123)
train_indices <- sample(nrow(sd1_data), 0.8 * nrow(sd1_data))
train_data <- sd1_data[train_indices, ]
test_data <- sd1_data[-train_indices, ]
```{r}
train_data <- train_data %>% rename(y = Y) # rinominato y piccolo perche pacchetto stan richiede cosi
train_data$y <- as.numeric(train_data$y)
uncertainty_bnn <- create_uncertainty_bnn(train_data, 'y')
# Funzione per creare ensemble che simula Monte Carlo sampling
create_uncertainty_bnn <- function(data, target_col, n_models = 3) {
models <- list()
# Crea formula dinamicamente
formula_str <- paste(target_col, "~ .")
for(i in 1:n_models) {
cat("Training model", i, "of", n_models, "\n")
# Bootstrap sampling (per aleatoric)
boot_indices <- sample(nrow(data), nrow(data), replace = TRUE)
boot_data <- data[boot_indices, ]
# Aggiungi noise ai parametri (per epistemic)
set.seed(i)  # diversi seed per diversi modelli
# SOSTITUITO: Allena BNN con bnns
model <- bnns(
formula = as.formula(formula_str),
data = boot_data,
hidden_layers = 10,  # varia architettura
epochs = 50,               # varia training
seed = i
)
models[[i]] <- model
}
return(models)
}
# Funzione per creare ensemble che simula Monte Carlo sampling
create_uncertainty_bnn <- function(data, target_col, n_models = 3) {
models <- list()
# Crea formula dinamicamente
formula_str <- paste(target_col, "~ .")
for(i in 1:n_models) {
cat("Training model", i, "of", n_models, "\n")
# Bootstrap sampling (per aleatoric)
boot_indices <- sample(nrow(data), nrow(data), replace = TRUE)
boot_data <- data[boot_indices, ]
# Aggiungi noise ai parametri (per epistemic)
set.seed(i)  # diversi seed per diversi modelli
# SOSTITUITO: Allena BNN con bnns
model <- bnns(
formula = as.formula(formula_str),
data = boot_data,
hidden_layers = 10,  # varia architettura
epochs = 50,               # varia training
seed = i
)
models[[i]] <- model
}
return(models)
}
calculate_group_accuracy <- function(models, test_data, sensitive_attr) {
groups <- unique(test_data[[sensitive_attr]])
results <- data.frame(
Group = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE
)
for (group in groups) {
group_data <- test_data[test_data[[sensitive_attr]] == group, ]
# MODIFICATO: Monte Carlo sampling con BNN
accuracies <- sapply(models, function(model) {
# 10 campioni Monte Carlo dalla posteriore bayesiana
mc_preds <- replicate(10, predict(model, group_data, type = "response"))
avg_pred <- rowMeans(mc_preds)
predicted_classes <- ifelse(avg_pred > 0.5, 1, 0)
mean(predicted_classes == group_data$Y)
})
results <- rbind(results, data.frame(
Group = as.character(group),
Accuracy = mean(accuracies)
))
}
return(results)
}
uncertainty_bnn <- create_uncertainty_bnn(train_data, 'y')
test_data$y <- as.numeric(test_data$Y)
print(test_data$y)
test_data$y_pred <- as.numeric(round(predict(models[[1]], newdata = test_data)))  # esempio con primo modello
test_data$y_pred <- as.numeric(round(predict(uncertainty_bnn[[1]], newdata = test_data)))  # esempio con primo modello
test_data$y_pred <- as.numeric(round(predict(uncertainty_bnn[[1]], newdata = test_data)))  # esempio con primo modello
print(uncertainty_bnn)
test_data$y_pred <- as.numeric(round(predict(uncertainty_bnn[[1]], newdata = test_data)))  # esempio con primo modello
features <- setdiff(names(train_data), "y")
features
features <- setdiff(names(train_data), "y")
test_data_clean <- test_data[, features]
test_data_clean <- test_data_clean[, names(train_data[, features])]
test_data$y_pred <- as.numeric(round(predict(uncertainty_bnn[[1]], newdata = test_data_clean)))  # esempio con primo modello
dim(models[[1]]$data$X)  # matrice delle feature usata nel training
dim(uncertainty_bnn[[1]]$data$X)  # matrice delle feature usata nel training
dim(test_data_clean)     # matrice delle feature nel test
names(train_data)
# Test rapido per identificare il problema
test_group <- test_data[1:5, ]  # Prendi solo 5 righe
model <- uncertainty_bnn[[1]]  # Prendi il primo modello
# Testa una singola predizione
single_pred <- predict(model, test_group, type = "response")
```{r}
test_data
train_data
# Assicurati che la colonna 'y' sia numerica anche nel test set
test_data$y <- as.numeric(test_data$y)
# Calcolo dell'accuratezza per gruppo sensibile 'G'
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G")
calculate_group_accuracy <- function(models, test_data, sensitive_attr) {
groups <- unique(test_data[[sensitive_attr]])
results <- data.frame(
Group = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE
)
for (group in groups) {
group_data <- test_data[test_data[[sensitive_attr]] == group, ]
group_data_x <- group_data %>% select(feature1, feature2)
true_labels <- group_data$y
accuracies <- sapply(models, function(model) {
mc_preds <- replicate(10, predict(model, group_data_x, type = "response"))
avg_pred <- rowMeans(mc_preds)
predicted_classes <- ifelse(avg_pred > 0.5, 1, 0)
mean(predicted_classes == true_labels)
})
results <- rbind(results, data.frame(
Group = as.character(group),
Accuracy = mean(accuracies)
))
}
return(results)
}
# Calcolo dell'accuratezza per gruppo sensibile 'G'
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G")
library(dplyr)
# Calcolo dell'accuratezza per gruppo sensibile 'G'
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G")
uncertainty_bnn <- create_uncertainty_bnn(train_data, 'y')
# Calcolo dell'accuratezza per gruppo sensibile 'G'
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G")
library(dplyr)
# Calcolo dell'accuratezza per gruppo sensibile 'G'
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G")
calculate_group_accuracy <- function(models, test_data, sensitive_attr) {
groups <- unique(test_data[[sensitive_attr]])
results <- data.frame(
Group = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE
)
for (group in groups) {
group_data <- test_data[test_data[[sensitive_attr]] == group, ]
group_data_x <- dplyr::select(group_data, feature1, feature2)
true_labels <- group_data$y
accuracies <- sapply(models, function(model) {
mc_preds <- replicate(10, predict(model, group_data_x, type = "response"))
avg_pred <- rowMeans(mc_preds)
predicted_classes <- ifelse(avg_pred > 0.5, 1, 0)
mean(predicted_classes == true_labels)
})
results <- rbind(results, data.frame(
Group = as.character(group),
Accuracy = mean(accuracies)
))
}
return(results)
}
# Calcolo dell'accuratezza per gruppo sensibile 'G'
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G")
model$feature_names
uncertainty_bnn$feature_names
View(uncertainty_bnn)
uncertainty_bnn[[1]]$feature_names
feature_names <- c("feature1", "feature2")
feature_names <- c("feature1", "feature2")
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G", feature_names)
calculate_group_accuracy <- function(models, test_data, sensitive_attr, feature_names) {
groups <- unique(test_data[[sensitive_attr]])
results <- data.frame(
Group = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE
)
for (group in groups) {
group_data <- test_data[test_data[[sensitive_attr]] == group, ]
true_labels <- group_data$y
accuracies <- sapply(models, function(model) {
group_data_x <- dplyr::select(group_data, all_of(feature_names))
mc_preds <- replicate(10, predict(model, group_data_x, type = "response"))
avg_pred <- rowMeans(mc_preds)
predicted_classes <- ifelse(avg_pred > 0.5, 1, 0)
mean(predicted_classes == true_labels)
})
results <- rbind(results, data.frame(
Group = as.character(group),
Accuracy = mean(accuracies)
))
}
return(results)
}
group_accuracy_results <- calculate_group_accuracy(uncertainty_bnn, test_data, sensitive_attr = "G", feature_names)
prepare_test_data <- function(train_data, test_data, target_col, sensitive_col) {
feature_names <- setdiff(names(train_data), c(target_col, sensitive_col))
# Seleziona solo le feature
test_x <- test_data[, feature_names]
# Assicurati che siano numeriche
test_x <- as.data.frame(lapply(test_x, as.numeric))
return(test_x)
}
calculate_group_accuracy <- function(models, test_data, sensitive_attr, train_data, target_col = "y") {
groups <- unique(test_data[[sensitive_attr]])
feature_names <- setdiff(names(train_data), c(target_col, sensitive_attr))
results <- data.frame(
Group = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE
)
for (group in groups) {
group_data <- test_data[test_data[[sensitive_attr]] == group, ]
true_labels <- group_data[[target_col]]
group_data_x <- group_data[, feature_names]
group_data_x <- as.data.frame(lapply(group_data_x, as.numeric))
accuracies <- sapply(models, function(model) {
mc_preds <- replicate(10, predict(model, group_data_x, type = "response"))
avg_pred <- rowMeans(mc_preds)
predicted_classes <- ifelse(avg_pred > 0.5, 1, 0)
mean(predicted_classes == true_labels)
})
results <- rbind(results, data.frame(
Group = as.character(group),
Accuracy = mean(accuracies)
))
}
return(results)
}
group_accuracy_results <- calculate_group_accuracy(
models = uncertainty_bnn,
test_data = test_data,
sensitive_attr = "G",
train_data = train_data,
target_col = "y"
)
# Seleziona il primo modello
model <- uncertainty_bnn[[1]]
# Seleziona un gruppo specifico, ad esempio G == 0
group_data <- test_data[test_data$G == 0, ]
# Seleziona solo le feature
group_data_x <- group_data[, c("feature1", "feature2")]
# Assicurati che siano numeriche
group_data_x <- as.data.frame(lapply(group_data_x, as.numeric))
# Etichette vere
true_labels <- group_data$y
# Etichette vere
true_labels <- group_data$y
# Prova una singola predizione Monte Carlo
mc_preds <- replicate(10, predict(model, group_data_x, type = "response"))
print(group_data_x)
# Dimensioni delle feature nel test set
print(dim(group_data_x))
# Dimensioni delle medie e deviazioni salvate nel modello
print(length(model$x_mean))
print(length(model$x_sd))
# Seleziona il primo modello
model <- uncertainty_bnn[[1]]
# Estrai le feature usate nel training (escludi 'y')
feature_names <- setdiff(names(train_data), "y")
# Seleziona i dati del gruppo G == 0
group_data <- test_data[test_data$G == 0, ]
# Seleziona le feature coerenti
group_data_x <- group_data[, feature_names]
group_data_x <- as.data.frame(lapply(group_data_x, as.numeric))
# Normalizzazione manuale con le statistiche del modello
group_data_x_norm <- sweep(group_data_x, 2, model$x_mean, "-")
group_data_x_norm <- sweep(group_data_x_norm, 2, model$x_sd, "/")
# Seleziona il primo modello
model <- uncertainty_bnn[[1]]
# Recupera le feature usate nel training
feature_names <- setdiff(names(model$data), "y")
# Seleziona i dati del gruppo G == 0
group_data <- test_data[test_data$G == 0, ]
# Seleziona le feature coerenti
group_data_x <- group_data[, feature_names]
